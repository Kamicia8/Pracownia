{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26a96c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d1e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(func, n_samples=1024):\n",
    "    x = np.linspace(-10, 10, n_samples).astype(np.float32)\n",
    "    y = func(x).astype(np.float32)\n",
    "    y = y.reshape(1, -1) \n",
    "    return torch.from_numpy(y).to(device), torch.from_numpy(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e77226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import diffusers.schedulers.scheduling_ddpm \n",
    "\n",
    "# def get_beta_schedule(T):\n",
    "#     return torch.linspace(1e-4, 0.02, T).to(device)\n",
    "\n",
    "def custom_beta_schedule(T, kind='cosine'):\n",
    "    if kind == 'cosine':\n",
    "        steps = torch.arange(T, dtype=torch.float32, device=device)\n",
    "        betas = (torch.cos(steps / T * (torch.pi / 2))) ** 2 * 0.02\n",
    "        return betas\n",
    "    elif kind == 'exp':\n",
    "        start = torch.log(torch.tensor(1e-4, device=device))\n",
    "        end = torch.log(torch.tensor(0.02, device=device))\n",
    "        betas = torch.exp(torch.linspace(start, end, T, device=device))\n",
    "        return betas\n",
    "    else:\n",
    "        return torch.linspace(1e-4, 0.02, T, device=device)\n",
    "\n",
    "\n",
    "\n",
    "# def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n",
    "#   if beta_schedule == 'quad':\n",
    "#     betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2\n",
    "#   elif beta_schedule == 'linear':\n",
    "#     betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
    "#   elif beta_schedule == 'warmup10':\n",
    "#     betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n",
    "#   elif beta_schedule == 'warmup50':\n",
    "#     betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n",
    "#   elif beta_schedule == 'const':\n",
    "#     betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
    "#   elif beta_schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "#     betas = 1. / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n",
    "#   else:\n",
    "#     raise NotImplementedError(beta_schedule)\n",
    "#   assert betas.shape == (num_diffusion_timesteps,)\n",
    "#   return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7645be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(x0, t, betas):\n",
    "    noise = torch.randn_like(x0)\n",
    "    sqrt_alpha_cumprod = torch.sqrt(torch.cumprod(1. - betas, dim=0)).to(device)\n",
    "    alpha_t = sqrt_alpha_cumprod[t].view(-1, 1)\n",
    "    xt = alpha_t * x0 + torch.sqrt(1 - alpha_t ** 2) * noise\n",
    "    return xt, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6680d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(signal_length=1024, n_func=10):\n",
    "    class UNet1D(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.time_embed = nn.Sequential(\n",
    "                nn.Embedding(1000, 64),\n",
    "                nn.Linear(64, signal_length)\n",
    "            )\n",
    "            self.func_embed = nn.Sequential(\n",
    "                nn.Embedding(n_func, 64),\n",
    "                nn.Linear(64, signal_length)\n",
    "            )\n",
    "            self.downs = nn.ModuleList([\n",
    "                nn.Sequential(nn.Conv1d(1, 32, kernel_size=4, stride=2, padding=1), nn.ReLU()),\n",
    "                nn.Sequential(nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1), nn.ReLU())\n",
    "            ])\n",
    "            self.mid = nn.Sequential(nn.Conv1d(64, 64, kernel_size=3, padding=1), nn.ReLU())\n",
    "            self.ups = nn.ModuleList([\n",
    "                nn.Sequential(nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1), nn.ReLU()),\n",
    "                nn.ConvTranspose1d(32, 1, kernel_size=4, stride=2, padding=1)\n",
    "            ])\n",
    "        def forward(self, x, t, func_id):\n",
    "            x = x.unsqueeze(1)\n",
    "            t_emb = self.time_embed(t).unsqueeze(1)\n",
    "            f_emb = self.func_embed(func_id).unsqueeze(1)\n",
    "            x = x + t_emb + f_emb\n",
    "            h1 = self.downs[0](x)\n",
    "            h2 = self.downs[1](h1)\n",
    "            h = self.mid(h2)\n",
    "            h = self.ups[0](h)\n",
    "            h = self.ups[1](h)\n",
    "            return h.squeeze(1)\n",
    "    return UNet1D().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, betas, T=1000, epochs=1000):\n",
    "    model.train()\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "        for x0, func_id in loader:\n",
    "            x0 = x0.to(device)\n",
    "            func_id = func_id.to(device).long()\n",
    "            t = torch.randint(0, T, (x0.size(0),), device=device)\n",
    "            xt, noise = forward_diffusion_sample(x0, t, betas)\n",
    "            pred = model(xt, t, func_id)\n",
    "            loss = F.mse_loss(pred, noise)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31417327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_signals(original, test, label):\n",
    "    mse = F.mse_loss(test, original).item()\n",
    "\n",
    "    print(f\"{label} mse: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7feb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def diff_process(model, func, betas, steps=100, x=None):\n",
    "    model.eval()\n",
    "    if x is None:\n",
    "        x = np.linspace(-10, 10, 1024).astype(np.float32)\n",
    "    y = func(x).astype(np.float32)\n",
    "    y_torch = torch.from_numpy(y).to(device).view(1, -1)\n",
    "\n",
    "    noisy_versions = []\n",
    "    noise = torch.randn_like(y_torch)\n",
    "    sqrt_alpha_cumprod = torch.sqrt(torch.cumprod(1 - betas, dim=0)).to(device)\n",
    "\n",
    "    for t in range(steps):\n",
    "        alpha_bar = sqrt_alpha_cumprod[t]\n",
    "        xt = alpha_bar * y_torch + torch.sqrt(1 - alpha_bar**2) * noise\n",
    "        noisy_versions.append(xt.detach().cpu().numpy()[0])\n",
    "\n",
    "    recovered_versions = []\n",
    "    xt = noisy_versions[-1]\n",
    "    xt = torch.from_numpy(xt).to(device).view(1, -1)\n",
    "\n",
    "    for t in reversed(range(steps)):\n",
    "        t_tensor = torch.tensor([t], device=device)\n",
    "        pred_noise = model(xt, t_tensor)\n",
    "        coef = 1 / torch.sqrt(1 - betas[t])\n",
    "        xt = (xt - betas[t] * pred_noise) * coef\n",
    "        recovered_versions.append(xt.detach().cpu().numpy()[0])\n",
    "\n",
    "    indices_to_show = [int(steps * i / 10) for i in range(11)]\n",
    "    if indices_to_show[-1] != steps-1:\n",
    "        indices_to_show[-1] = steps-1\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axs = plt.subplots(2, 11, figsize=(33, 6), sharey=True)\n",
    "\n",
    "    for i, idx in enumerate(indices_to_show):\n",
    "        axs[0, i].plot(x, y, label='Oryginał', linewidth=1, color='black')\n",
    "        axs[0, i].plot(x, noisy_versions[idx], label=f\"Krok {idx}\", color='blue')\n",
    "        axs[0, i].set_title(f\"Zaszum {idx}\")\n",
    "        axs[0, i].set_xticks([])\n",
    "        axs[0, i].set_yticks([])\n",
    "\n",
    "        axs[1, i].plot(x, y, label='Oryginał', linewidth=1, color='black')\n",
    "        axs[1, i].plot(x, recovered_versions[idx], label=f\"Krok {idx}\", color='red')\n",
    "        axs[1, i].set_title(f\"Odszum {idx}\")\n",
    "        axs[1, i].set_xticks([])\n",
    "        axs[1, i].set_yticks([])\n",
    "\n",
    "    axs[0, 0].legend(loc='upper right')\n",
    "\n",
    "    original = torch.from_numpy(y).to(device).view(1, -1)\n",
    "    noisy_final = torch.from_numpy(noisy_versions[-1]).to(device).view(1, -1)\n",
    "    denoised_final = torch.from_numpy(recovered_versions[-1]).to(device).view(1, -1)\n",
    "\n",
    "    compare_signals(original, noisy_final, label='zaszumiony (ostatni krok)')\n",
    "    compare_signals(original, denoised_final, label='odszumiony (ostatni krok)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54473dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_sin(x): return np.sin(x) \n",
    "def f_tan(x): return np.tan(x)\n",
    "def f_sgn(x): return np.sign(x)\n",
    "def f_sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def f_relu(x): return np.maximum(0, x)\n",
    "def f_log10(x): return np.log10(np.clip(x, 1e-3, None))\n",
    "def f_log2(x): return np.log2(np.clip(x, 1e-3, None))\n",
    "def f_inv(x): return 1 / np.clip(x, 1e-3, None)\n",
    "def f_exp(x): return np.exp(x)\n",
    "def f_poly(x): return x**2 + 2*x + 1\n",
    "def f_sin_1x(x): return np.sin(1/x)\n",
    "def f_sin_2(x): return np.sin(x)**2\n",
    "\n",
    "f_names =  [(\"sin\", f_sin),(\"tan\", f_tan),(\"exp\", f_exp),(\"log10\", f_log10),(\"poly\", f_poly),\n",
    "            (\"sigmoid\", f_sigmoid),(\"relu\", f_relu),(\"inv\", f_inv),(\"sin_1x\", f_sin_1x),(\"zin_2\", f_sin_2)] #(\"log2\", f_log2),\n",
    "\n",
    "\n",
    "# f_names =  [(\"log10\", f_log10),(\"log2\", f_log2),(\"poly\", f_poly)]\n",
    "\n",
    "f_index = {name: idx for idx, (name, _) in enumerate(f_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_dataset(f_names):\n",
    "    xs = []\n",
    "    funcs = []\n",
    "    for i, (name, func) in enumerate(f_names):\n",
    "        y, x = get_dataset(func)\n",
    "        xs.append(y)\n",
    "        funcs.append(torch.full((y.shape[0],), i, dtype=torch.long)) \n",
    "    xs = torch.cat(xs, dim=0)\n",
    "    funcs = torch.cat(funcs, dim=0)\n",
    "    return TensorDataset(xs, funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet_general_T100_ep1000_kindcosine\n",
      "unet_general_T100_ep1000_kindexp\n",
      "unet_general_T100_ep1000_kindlinear\n",
      "unet_general_T250_ep1000_kindcosine\n",
      "unet_general_T250_ep1000_kindexp\n",
      "unet_general_T250_ep1000_kindlinear\n",
      "unet_general_T500_ep1000_kindcosine\n",
      "unet_general_T500_ep1000_kindexp\n",
      "unet_general_T500_ep1000_kindlinear\n",
      "unet_general_T1000_ep1000_kindcosine\n",
      "unet_general_T1000_ep1000_kindexp\n",
      "unet_general_T1000_ep1000_kindlinear\n",
      "unet_general_T100_ep5000_kindcosine\n",
      "unet_general_T100_ep5000_kindexp\n",
      "unet_general_T100_ep5000_kindlinear\n",
      "unet_general_T250_ep5000_kindcosine\n",
      "unet_general_T250_ep5000_kindexp\n",
      "unet_general_T250_ep5000_kindlinear\n",
      "unet_general_T500_ep5000_kindcosine\n",
      "unet_general_T500_ep5000_kindexp\n",
      "unet_general_T500_ep5000_kindlinear\n",
      "unet_general_T1000_ep5000_kindcosine\n",
      "unet_general_T1000_ep5000_kindexp\n",
      "unet_general_T1000_ep5000_kindlinear\n",
      "unet_general_T100_ep10000_kindcosine\n",
      "unet_general_T100_ep10000_kindexp\n",
      "unet_general_T100_ep10000_kindlinear\n",
      "unet_general_T250_ep10000_kindcosine\n",
      "unet_general_T250_ep10000_kindexp\n",
      "unet_general_T250_ep10000_kindlinear\n",
      "unet_general_T500_ep10000_kindcosine\n",
      "unet_general_T500_ep10000_kindexp\n",
      "unet_general_T500_ep10000_kindlinear\n",
      "unet_general_T1000_ep10000_kindcosine\n",
      "unet_general_T1000_ep10000_kindexp\n",
      "unet_general_T1000_ep10000_kindlinear\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    log_every = 200  \n",
    "\n",
    "    epochs = [1000, 5000, 10000]\n",
    "\n",
    "    Ts=[100, 250, 500, 1000]\n",
    "\n",
    "    for epoch in epochs:\n",
    "        for ts in Ts:\n",
    "            betas_cosine = custom_beta_schedule(ts, kind='cosine')\n",
    "            betas_exp = custom_beta_schedule(ts, kind='exp')\n",
    "            betas_linear = custom_beta_schedule(ts, kind='linear')\n",
    "            betas =[(\"cosine\", betas_cosine), (\"exp\", betas_exp), (\"linear\", betas_linear)]\n",
    "            for beta_name, beta in betas:\n",
    "                betas = custom_beta_schedule(ts, kind='cosine')\n",
    "                dataset = get_combined_dataset(f_names)\n",
    "                model = get_model()\n",
    "             \n",
    "                train(model, dataset, beta, T=ts, epochs=epoch)\n",
    "                # diff_process(model, func[1], beta, steps=ts, x=x.cpu().numpy())\n",
    "                print(f\"unet_general_T{ts}_ep{epoch}_kind{beta_name}\")\n",
    "\n",
    "                \n",
    "                torch.save(model.state_dict(), f\"unet_general_T{ts}_ep{epoch}_kind{beta_name}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
