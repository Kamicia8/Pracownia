{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a96c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.signal \n",
    "import scipy.ndimage \n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(func, n_samples=1024):\n",
    "    x = np.linspace(-10, 10, n_samples).astype(np.float32)\n",
    "    y = func(x).astype(np.float32)\n",
    "    y = y.reshape(1, -1) \n",
    "    return torch.from_numpy(y).to(device), torch.from_numpy(x).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e77226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import diffusers.schedulers.scheduling_ddpm \n",
    "\n",
    "# def get_beta_schedule(T):\n",
    "#     return torch.linspace(1e-4, 0.02, T).to(device)\n",
    "\n",
    "def betas_list(T, kind='cosine'):\n",
    "    if kind == 'cosine':\n",
    "        steps = torch.arange(T, dtype=torch.float32, device=device)\n",
    "        betas = (torch.cos(steps / T * (torch.pi / 2))) ** 2 * 0.01\n",
    "        return betas\n",
    "    elif kind == 'exp':\n",
    "        start = torch.log(torch.tensor(1e-4, device=device))\n",
    "        end = torch.log(torch.tensor(0.01, device=device))\n",
    "        betas = torch.exp(torch.linspace(start, end, T, device=device))\n",
    "        return betas\n",
    "    else:\n",
    "        return torch.linspace(1e-4, 0.01, T, device=device)\n",
    "\n",
    "\n",
    "\n",
    "# def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps): #TODO hugging face\n",
    "#   if beta_schedule == 'quad':\n",
    "#     betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2\n",
    "#   elif beta_schedule == 'linear':\n",
    "#     betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
    "#   elif beta_schedule == 'warmup10':\n",
    "#     betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n",
    "#   elif beta_schedule == 'warmup50':\n",
    "#     betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n",
    "#   elif beta_schedule == 'const':\n",
    "#     betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
    "#   elif beta_schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "#     betas = 1. / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n",
    "#   else:\n",
    "#     raise NotImplementedError(beta_schedule)\n",
    "#   assert betas.shape == (num_diffusion_timesteps,)\n",
    "#   return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(x0, t, betas):\n",
    "    noise = torch.randn_like(x0)\n",
    "    sqrt_alpha_cumprod = torch.sqrt(torch.cumprod(1. - betas, dim=0)).to(device)\n",
    "    alpha_t = sqrt_alpha_cumprod[t].view(-1, 1)\n",
    "    xt = alpha_t * x0 + torch.sqrt(1 - alpha_t ** 2) * noise\n",
    "    return xt, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6680d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, in_channels=1, base_channels=32, signal_length=1024, time_emb_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Embedding(1000, time_emb_dim),          \n",
    "            nn.Linear(time_emb_dim, signal_length)       \n",
    "        )\n",
    "\n",
    "        self.downs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels, base_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(base_channels, base_channels*2, kernel_size=4, stride=2, padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        ])\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv1d(base_channels*2, base_channels*2, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ups = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(base_channels*2, base_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.ReLU()\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose1d(base_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        x = x.unsqueeze(1) \n",
    "\n",
    "        t_emb = self.time_embed(t)         \n",
    "        t_emb = t_emb.unsqueeze(1)         \n",
    "        x = x + t_emb                     \n",
    "\n",
    "        h1 = self.downs[0](x)\n",
    "        h2 = self.downs[1](h1)\n",
    "        h = self.mid(h2)\n",
    "        h = self.ups[0](h)\n",
    "        h = self.ups[1](h)\n",
    "        return h.squeeze(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, betas, T=100, epochs=1000, log_every=100, loss_fn=F.mse_loss):\n",
    "    model.train()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for (x0,) in dataloader:\n",
    "            t = torch.randint(0, T, (x0.size(0),), device=device)\n",
    "            xt, noise = forward_diffusion_sample(x0, t, betas)\n",
    "            pred = model(xt, t)\n",
    "            loss = loss_fn(pred, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # if epoch % log_every == 0 or epoch == epochs - 1:\n",
    "        #     print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31417327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_signals(original, test, label):\n",
    "#     mse = F.mse_loss(test, original).item()\n",
    "\n",
    "#     print(f\"{label} mse: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7feb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def diff_process(model, func, betas, steps=100, x=None):\n",
    "    model.eval()\n",
    "    if x is None:\n",
    "        x = np.linspace(-10, 10, 1024).astype(np.float32)\n",
    "    y = func(x).astype(np.float32)\n",
    "    y_torch = torch.from_numpy(y).to(device).view(1, -1)\n",
    "\n",
    "    noisy_versions = []\n",
    "    noise = torch.randn_like(y_torch)\n",
    "    sqrt_alpha_cumprod = torch.sqrt(torch.cumprod(1 - betas, dim=0)).to(device)\n",
    "\n",
    "    for t in range(steps):\n",
    "        alpha_bar = sqrt_alpha_cumprod[t]\n",
    "        xt = alpha_bar * y_torch + torch.sqrt(1 - alpha_bar**2) * noise\n",
    "        noisy_versions.append(xt.detach().cpu().numpy()[0])\n",
    "\n",
    "    recovered_versions = []\n",
    "    xt = noisy_versions[-1]\n",
    "    xt = torch.from_numpy(xt).to(device).view(1, -1)\n",
    "\n",
    "    for t in reversed(range(steps)):\n",
    "        t_tensor = torch.tensor([t], device=device)\n",
    "        pred_noise = model(xt, t_tensor)\n",
    "        coef = 1 / torch.sqrt(1 - betas[t])\n",
    "        xt = (xt - betas[t] * pred_noise) * coef\n",
    "        recovered_versions.append(xt.detach().cpu().numpy()[0])\n",
    "\n",
    "    indices_to_show = [int(steps * i / 10) for i in range(11)]\n",
    "    if indices_to_show[-1] != steps-1:\n",
    "        indices_to_show[-1] = steps-1\n",
    "\n",
    "    fig, axs = plt.subplots(2, 11, figsize=(33, 6), sharey=True)\n",
    "\n",
    "    for i, idx in enumerate(indices_to_show):\n",
    "        axs[0, i].plot(x, y, label='Oryginał', linewidth=1, color='black')\n",
    "        axs[0, i].plot(x, noisy_versions[idx], label=f\"Krok {idx}\", color='blue')\n",
    "        axs[0, i].set_title(f\"Zaszum {idx}\")\n",
    "        axs[0, i].set_xticks([])\n",
    "        axs[0, i].set_yticks([])\n",
    "\n",
    "        axs[1, i].plot(x, y, label='Oryginał', linewidth=1, color='black')\n",
    "        axs[1, i].plot(x, recovered_versions[idx], label=f\"Odszum {idx}\", color='red')\n",
    "        axs[1, i].set_title(f\"Odszum {idx}\")\n",
    "        axs[1, i].set_xticks([])\n",
    "        axs[1, i].set_yticks([])\n",
    "\n",
    "    axs[0, 0].legend(loc='upper right')\n",
    "\n",
    "    original = torch.from_numpy(y).to(device).view(1, -1)\n",
    "    noisy_final = torch.from_numpy(noisy_versions[-1]).to(device).view(1, -1)\n",
    "    denoised_final = torch.from_numpy(recovered_versions[-1]).to(device).view(1, -1)\n",
    "\n",
    "    # compare_signals(original, noisy_final, label='zaszumiony (ostatni krok)')\n",
    "    # compare_signals(original, denoised_final, label='odszumiony (ostatni krok)')\n",
    "\n",
    "    original_np = original.cpu().numpy()\n",
    "    denoised_np = denoised_final.cpu().numpy()\n",
    "    noisy_final_np = noisy_final.cpu().numpy()\n",
    "    x_np =  x # x.cpu().numpy()\n",
    "\n",
    "    mse_noisy = np.mean((original_np - noisy_final_np) ** 2)\n",
    "    rmse_noisy = np.sqrt(mse_noisy) \n",
    "    mae_noisy = np.mean(np.abs(original_np - noisy_final_np))\n",
    "    mape_noisy = np.mean(np.abs((original_np - noisy_final_np) / (original_np + 1e-8))) * 100\n",
    "    # rse_noisy = np.sum((original_np - noisy_final_np) ** 2) / (np.sum((original_np - np.mean(original_np)) ** 2) + 1e-8)\n",
    "\n",
    "    mse = np.mean((original_np - denoised_np) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(original_np - denoised_np))\n",
    "    mape = np.mean(np.abs((original_np - denoised_np) / (original_np + 1e-8))) * 100\n",
    "    # rse = np.sum((original_np - denoised_np) ** 2) / (np.sum((original_np - np.mean(original_np)) ** 2) + 1e-8)\n",
    "\n",
    "    # print(f\"original-noisy ----- MSE: {mse_noisy:.6f}; RMSE: {rmse_noisy:.6f}; MAE: {mae_noisy:.6f}; MAPE: {mape_noisy:.6f}%\")\n",
    "\n",
    "    # print(f\"original-denoised ----- MSE: {mse:.6f}; RMSE: {rmse:.6f}; MAE: {mae:.6f}; MAPE: {mape:.6f}%\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig_smooth, axs_smooth = plt.subplots(1, 3, figsize=(14, 4), sharex=True, sharey=True) \n",
    "    axs_smooth[0].plot(x_np, original_np[0], label='Oryginał', color='black')\n",
    "    axs_smooth[0].plot(x_np, denoised_np[0], label='Odszumiony', color='red')\n",
    "    axs_smooth[0].set_title('Oryginał vs odszumiony')\n",
    "    axs_smooth[0].legend()\n",
    "\n",
    "    sigma_gaussian = 5 \n",
    "    smoothed_gaussian = scipy.ndimage.gaussian_filter1d(denoised_np[0], sigma=sigma_gaussian)\n",
    "    axs_smooth[1].plot(x_np, original_np[0], label='Oryginał', color='black')\n",
    "    axs_smooth[1].plot(x_np, smoothed_gaussian, label=f'Wygładzony (Gauss, $\\\\sigma$={sigma_gaussian})', color='green')\n",
    "    axs_smooth[1].set_title(f'Wygładzenie Gaussem ($\\sigma$={sigma_gaussian})')\n",
    "    axs_smooth[1].legend()\n",
    "\n",
    "    spline_s_param = 0.8 \n",
    "    spl = UnivariateSpline(x_np, denoised_np[0], s=spline_s_param)\n",
    "    smoothed_spline = spl(x_np)\n",
    "    axs_smooth[2].plot(x_np, original_np[0], label='Oryginał', color='black')\n",
    "    axs_smooth[2].plot(x_np, smoothed_spline, label=f'Aproksymacja splajnem (s={spline_s_param})', color='purple')\n",
    "    axs_smooth[2].set_title(f'Aproksymacja splajnem (s={spline_s_param})')\n",
    "    axs_smooth[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    mse_gaussian = np.mean((original_np[0] - smoothed_gaussian) ** 2)\n",
    "    mse_spline = np.mean((original_np[0] - smoothed_spline) ** 2)\n",
    "\n",
    "    # print(f\"MSE po Gaussie: {mse_gaussian:.6f}\")\n",
    "    # print(f\"MSE po splajach: {mse_spline:.6f}\")\n",
    "\n",
    "    return {\n",
    "       'mse_noisy': mse_noisy, \n",
    "        'rmse_noisy': rmse_noisy,\n",
    "        'mae_noisy': mae_noisy,\n",
    "        'mape_noisy': mape_noisy,\n",
    "        # 'rse_noisy': rse_noisy,\n",
    "\n",
    "        'mse_denoised': mse, \n",
    "        'rmse_denoised': rmse,\n",
    "        'mae_denoised': mae,\n",
    "        'mape_denoised': mape,\n",
    "        # 'rse_denoised': rse,\n",
    "\n",
    "        'mse_gaussian': mse_gaussian,\n",
    "        'mse_spline': mse_spline\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54473dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_sin(x): return np.sin(x) \n",
    "def f_tan(x): return np.tan(x)\n",
    "def f_sgn(x): return np.sign(x)\n",
    "def f_sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def f_relu(x): return np.maximum(0, x)\n",
    "def f_log10(x): return np.log10(np.clip(x, 1e-3, None))\n",
    "def f_log2(x): return np.log2(np.clip(x, 1e-3, None))\n",
    "def f_inv(x): return 1 / np.clip(x, 1e-3, None)\n",
    "def f_exp(x): return np.exp(x)\n",
    "def f_poly(x): return x**2 + 2*x + 1\n",
    "def f_sin_1x(x): return np.sin(1/x)\n",
    "def f_sin_2(x): return np.sin(x)**2\n",
    "\n",
    "f_names =  [(\"sin\", f_sin),(\"exp\", f_exp),(\"log10\", f_log10),(\"poly\", f_poly),\n",
    "            (\"sigmoid\", f_sigmoid),(\"relu\", f_relu),(\"inv\", f_inv),(\"sin_1x\", f_sin_1x),(\"zin_2\", f_sin_2)] #(\"log2\", f_log2),\n",
    "\n",
    "\n",
    "# f_names =  [(\"log10\", f_log10),(\"log2\", f_log2),(\"poly\", f_poly)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b28b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_result(result, path=\"results10000.json\"):\n",
    "    for key, value in result.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            result[key] = value.tolist()  \n",
    "        elif isinstance(value, np.float32):\n",
    "            result[key] = float(value)  \n",
    "\n",
    "    results = []\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            try:\n",
    "                content = f.read().strip()\n",
    "                if content: \n",
    "                    results = json.loads(content)\n",
    "                else:\n",
    "                    results = [] \n",
    "            except json.JSONDecodeError:\n",
    "                results = [] \n",
    "    results.append(result)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(pred, target):\n",
    "    return torch.sqrt(F.mse_loss(pred, target))\n",
    "\n",
    "def mape_loss(pred, target, epsilon=1e-8): #bo nie wolno przez 0\n",
    "    return torch.mean(torch.abs((target - pred) / (target + epsilon))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    log_every = 200\n",
    "\n",
    "    epochs = [10000]\n",
    "    Ts = [100, 350, 800]\n",
    "\n",
    "    loss_functions_for_training = {\n",
    "        \"MSE\": F.mse_loss,\n",
    "    }\n",
    "\n",
    "    for epoch in epochs:\n",
    "        for ts in Ts:\n",
    "            betas_cosine = betas_list(ts, kind='cosine')\n",
    "            betas_exp = betas_list(ts, kind='exp')\n",
    "            betas_linear = betas_list(ts, kind='linear')\n",
    "            betas = [(\"cosine\", betas_cosine), (\"exp\", betas_exp), (\"linear\", betas_linear)]\n",
    "            for beta_name, beta in betas:\n",
    "                for loss_name_train, current_loss_fn_train in loss_functions_for_training.items():\n",
    "                    for func_name, func_fn in f_names: \n",
    "                        y, x = get_dataset(func_fn)\n",
    "                        dataset = TensorDataset(y)\n",
    "\n",
    "                        model = UNet1D().to(device)\n",
    "\n",
    "                        print(f\"-----------function {func_name}---------, epochs: {epoch}, T: {ts}, beta type:{beta_name}, training_loss: {loss_name_train}-----------\")\n",
    "                        train(model, dataset, beta, T=ts, epochs=epoch, log_every=log_every, loss_fn=current_loss_fn_train)\n",
    "\n",
    "                        metrics = diff_process(model, func_fn, beta, steps=ts, x=x.cpu().numpy())\n",
    "                        result = {\n",
    "                            \"function\": func_name,\n",
    "                            \"beta\": beta_name,\n",
    "                            \"T\": ts,\n",
    "                            \"epochs\": epoch,\n",
    "                            \"training_loss_function\": loss_name_train, \n",
    "                            **metrics\n",
    "                        }\n",
    "                        save_result(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    log_every = 200\n",
    "\n",
    "    epochs = [10000]\n",
    "    Ts = [100, 350, 800]\n",
    "\n",
    "    loss_functions_for_training = {\n",
    "        \"RMSE\": rmse_loss\n",
    "    }\n",
    "\n",
    "    for epoch in epochs:\n",
    "        for ts in Ts:\n",
    "            betas_cosine = betas_list(ts, kind='cosine')\n",
    "            betas_exp = betas_list(ts, kind='exp')\n",
    "            betas_linear = betas_list(ts, kind='linear')\n",
    "            betas = [(\"cosine\", betas_cosine), (\"exp\", betas_exp), (\"linear\", betas_linear)]\n",
    "            for beta_name, beta in betas:\n",
    "                for loss_name_train, current_loss_fn_train in loss_functions_for_training.items():\n",
    "                    for func_name, func_fn in f_names: \n",
    "                        y, x = get_dataset(func_fn)\n",
    "                        dataset = TensorDataset(y)\n",
    "\n",
    "                        model = UNet1D().to(device)\n",
    "\n",
    "                        print(f\"-----------function {func_name}---------, epochs: {epoch}, T: {ts}, beta type:{beta_name}, training_loss: {loss_name_train}-----------\")\n",
    "                        train(model, dataset, beta, T=ts, epochs=epoch, log_every=log_every, loss_fn=current_loss_fn_train)\n",
    "\n",
    "                        metrics = diff_process(model, func_fn, beta, steps=ts, x=x.cpu().numpy())\n",
    "                        result = {\n",
    "                            \"function\": func_name,\n",
    "                            \"beta\": beta_name,\n",
    "                            \"T\": ts,\n",
    "                            \"epochs\": epoch,\n",
    "                            \"training_loss_function\": loss_name_train, \n",
    "                            **metrics\n",
    "                        }\n",
    "                        save_result(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e45512",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    log_every = 200\n",
    "\n",
    "    epochs = [10000]\n",
    "    Ts = [100, 350, 800]\n",
    "\n",
    "    loss_functions_for_training = { \n",
    "        \"MAE\": F.l1_loss\n",
    "    }\n",
    "\n",
    "    for epoch in epochs:\n",
    "        for ts in Ts:\n",
    "            betas_cosine = betas_list(ts, kind='cosine')\n",
    "            betas_exp = betas_list(ts, kind='exp')\n",
    "            betas_linear = betas_list(ts, kind='linear')\n",
    "            betas = [(\"cosine\", betas_cosine), (\"exp\", betas_exp), (\"linear\", betas_linear)]\n",
    "            for beta_name, beta in betas:\n",
    "                for loss_name_train, current_loss_fn_train in loss_functions_for_training.items():\n",
    "                    for func_name, func_fn in f_names: \n",
    "                        y, x = get_dataset(func_fn)\n",
    "                        dataset = TensorDataset(y)\n",
    "\n",
    "                        model = UNet1D().to(device)\n",
    "\n",
    "                        print(f\"-----------function {func_name}---------, epochs: {epoch}, T: {ts}, beta type:{beta_name}, training_loss: {loss_name_train}-----------\")\n",
    "                        train(model, dataset, beta, T=ts, epochs=epoch, log_every=log_every, loss_fn=current_loss_fn_train)\n",
    "\n",
    "                        metrics = diff_process(model, func_fn, beta, steps=ts, x=x.cpu().numpy())\n",
    "                        result = {\n",
    "                            \"function\": func_name,\n",
    "                            \"beta\": beta_name,\n",
    "                            \"T\": ts,\n",
    "                            \"epochs\": epoch,\n",
    "                            \"training_loss_function\": loss_name_train, \n",
    "                            **metrics\n",
    "                        }\n",
    "                        save_result(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    log_every = 200\n",
    "\n",
    "    epochs = [10000]\n",
    "    Ts = [100, 350, 800]\n",
    "\n",
    "    loss_functions_for_training = {\n",
    "        \"MAPE\": mape_loss\n",
    "    }\n",
    "\n",
    "    for epoch in epochs:\n",
    "        for ts in Ts:\n",
    "            betas_cosine = betas_list(ts, kind='cosine')\n",
    "            betas_exp = betas_list(ts, kind='exp')\n",
    "            betas_linear = betas_list(ts, kind='linear')\n",
    "            betas = [(\"cosine\", betas_cosine), (\"exp\", betas_exp), (\"linear\", betas_linear)]\n",
    "            for beta_name, beta in betas:\n",
    "                for loss_name_train, current_loss_fn_train in loss_functions_for_training.items():\n",
    "                    for func_name, func_fn in f_names: \n",
    "                        y, x = get_dataset(func_fn)\n",
    "                        dataset = TensorDataset(y)\n",
    "\n",
    "                        model = UNet1D().to(device)\n",
    "\n",
    "                        print(f\"-----------function {func_name}---------, epochs: {epoch}, T: {ts}, beta type:{beta_name}, training_loss: {loss_name_train}-----------\")\n",
    "                        train(model, dataset, beta, T=ts, epochs=epoch, log_every=log_every, loss_fn=current_loss_fn_train)\n",
    "\n",
    "                        metrics = diff_process(model, func_fn, beta, steps=ts, x=x.cpu().numpy())\n",
    "                        result = {\n",
    "                            \"function\": func_name,\n",
    "                            \"beta\": beta_name,\n",
    "                            \"T\": ts,\n",
    "                            \"epochs\": epoch,\n",
    "                            \"training_loss_function\": loss_name_train, \n",
    "                            **metrics\n",
    "                        }\n",
    "                        save_result(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
